#!/usr/bin/env python3
"""
Salesforce Attachments Extract - Main Entry Point

Runs the complete workflow:
1. Query attachments using sf CLI
2. Download attachment files using REST API
"""

import os
import sys
import csv
import subprocess
import logging
from math import ceil
from pathlib import Path
from typing import Optional, List, Dict
from dotenv import load_dotenv

from src.downloader import download_attachments, setup_logging
from src.filters import parse_filter_config, ParentIdFilter, apply_parent_id_filter

logger = logging.getLogger(__name__)


def run_query_script(
    org_alias: str,
    output_dir: Path,
    query_limit: int = 100,
    offset: int = 0
) -> Path:
    """
    Run the bash script to query attachments.

    Args:
        org_alias: Salesforce org alias
        output_dir: Directory to save metadata CSV
        query_limit: Maximum number of records to query (default: 100)
        offset: SOQL OFFSET for pagination (default: 0)

    Returns:
        Path to the generated CSV file

    Raises:
        RuntimeError: If query script fails
        FileNotFoundError: If script or CSV not found
    """
    logger.info("Running attachment query script...")
    logger.info(f"Query limit: {query_limit}, offset: {offset}")

    script_path = Path(__file__).parent / 'scripts' / 'query_attachments.sh'

    if not script_path.exists():
        raise FileNotFoundError(
            f"Query script not found at path: {script_path.absolute()}. "
            f"Please ensure the scripts directory and query_attachments.sh exist."
        )

    # Run script with query limit and offset parameters
    result = subprocess.run(
        ['bash', str(script_path), org_alias, str(output_dir), str(query_limit), str(offset)],
        capture_output=True,
        text=True
    )

    if result.returncode != 0:
        logger.error(f"Query script failed: {result.stderr}")
        raise RuntimeError("Failed to query attachments")

    logger.info(result.stdout)

    # Find the generated CSV file (most recent in output_dir)
    csv_files = sorted(output_dir.glob('attachments_*.csv'), key=lambda p: p.stat().st_mtime)

    if not csv_files:
        raise FileNotFoundError(
            f"No CSV file generated by query script in directory: {output_dir.absolute()}. "
            f"The query script may have failed or no attachments were found."
        )

    return csv_files[-1]  # Return most recent


def run_query_script_with_filter(
    org_alias: str,
    output_dir: Path,
    where_clause: str
) -> Path:
    """
    Run the bash script to query attachments with a WHERE clause filter.

    This function is used by the CSV-records workflow to query attachments
    for specific ParentIds using a pre-built WHERE clause (e.g., WHERE ParentId IN (...)).

    Args:
        org_alias: Salesforce org alias
        output_dir: Directory to save metadata CSV
        where_clause: Pre-built WHERE clause (e.g., "WHERE ParentId IN ('id1','id2')")

    Returns:
        Path to the generated CSV file

    Raises:
        RuntimeError: If query script fails
        FileNotFoundError: If script or CSV not found
    """
    logger.info("Running attachment query script with filter...")
    logger.debug(f"WHERE clause: {where_clause[:100]}...")  # Log first 100 chars

    script_path = Path(__file__).parent / 'scripts' / 'query_attachments.sh'

    if not script_path.exists():
        raise FileNotFoundError(
            f"Query script not found at path: {script_path.absolute()}. "
            f"Please ensure the scripts directory and query_attachments.sh exist."
        )

    # Run script with WHERE clause as 5th parameter
    # Parameters: org_alias, output_dir, query_limit (0), offset (0), where_clause
    result = subprocess.run(
        ['bash', str(script_path), org_alias, str(output_dir), '0', '0', where_clause],
        capture_output=True,
        text=True
    )

    if result.returncode != 0:
        logger.error(f"Query script failed: {result.stderr}")
        raise RuntimeError("Failed to query attachments with filter")

    logger.info(result.stdout)

    # Find the generated CSV file (most recent in output_dir)
    csv_files = sorted(output_dir.glob('attachments_*.csv'), key=lambda p: p.stat().st_mtime)

    if not csv_files:
        raise FileNotFoundError(
            f"No CSV file generated by query script in directory: {output_dir.absolute()}. "
            f"The query script may have failed or no attachments were found."
        )

    return csv_files[-1]  # Return most recent


def run_paginated_query(
    org_alias: str,
    output_dir: Path,
    target_count: int,
    batch_size: int,
    target_mode: str = 'exact',
    filter_config: Optional[ParentIdFilter] = None
) -> Path:
    """
    Run paginated queries to fetch target number of attachment records.

    Uses SOQL OFFSET to paginate through results. For Python filter strategy,
    continues fetching until enough filtered matches are found.

    Args:
        org_alias: Salesforce org alias
        output_dir: Directory to save merged metadata CSV
        target_count: Target number of records to fetch
        batch_size: Number of records per query (QUERY_LIMIT)
        target_mode: 'exact' (trim to target) or 'minimum' (at least target)
        filter_config: Optional filter configuration for ParentId filtering

    Returns:
        Path to the merged CSV file with all paginated records

    Raises:
        RuntimeError: If queries fail or OFFSET limit exceeded
        ValueError: If parameters are invalid
    """
    if target_count <= 0:
        raise ValueError(f"TARGET_COUNT must be positive, got {target_count}")

    if batch_size <= 0:
        raise ValueError(f"QUERY_LIMIT must be positive, got {batch_size}")

    # SOQL OFFSET limit
    MAX_OFFSET = 2000

    logger.info("=" * 70)
    logger.info("PAGINATION MODE ENABLED")
    logger.info("=" * 70)
    logger.info(f"Target count: {target_count}")
    logger.info(f"Batch size: {batch_size}")
    logger.info(f"Target mode: {target_mode}")

    if filter_config and filter_config.has_filters():
        logger.info(f"Filter config: {filter_config}")

    # Create metadata directory
    output_dir.mkdir(parents=True, exist_ok=True)

    accumulated_records: List[Dict[str, str]] = []
    offset = 0
    batch_num = 1
    csv_fieldnames: Optional[List[str]] = None

    # Determine if we need to apply Python filtering
    needs_python_filter = (
        filter_config and
        filter_config.has_filters() and
        filter_config.strategy == 'python'
    )

    while True:
        # Check OFFSET limit
        if offset > MAX_OFFSET:
            logger.error(
                f"SOQL OFFSET limit exceeded ({MAX_OFFSET}). "
                f"Cannot fetch more records. Consider reducing TARGET_COUNT."
            )
            raise RuntimeError(f"OFFSET limit {MAX_OFFSET} exceeded")

        # Optimize: if target already reached and not using Python filters, stop
        if not needs_python_filter and len(accumulated_records) >= target_count:
            logger.info(f"Target count {target_count} reached. Stopping pagination.")
            break

        # For Python filters, check filtered count
        if needs_python_filter:
            filtered_count = len(accumulated_records)
            if filtered_count >= target_count:
                logger.info(
                    f"Filtered target count {target_count} reached "
                    f"({filtered_count} filtered records). Stopping pagination."
                )
                break

        logger.info(f"Fetching batch {batch_num} (OFFSET {offset}, LIMIT {batch_size})...")

        try:
            # Execute query for this batch
            batch_csv_path = run_query_script(
                org_alias=org_alias,
                output_dir=output_dir,
                query_limit=batch_size,
                offset=offset
            )

            # Read batch records
            batch_records = []
            with batch_csv_path.open('r', encoding='utf-8') as f:
                reader = csv.DictReader(f)

                # Capture fieldnames from first batch
                if csv_fieldnames is None:
                    csv_fieldnames = reader.fieldnames

                for row in reader:
                    batch_records.append(row)

            logger.info(f"Batch {batch_num}: Retrieved {len(batch_records)} records")

            # If empty batch, no more records available
            if len(batch_records) == 0:
                logger.info("Empty batch received. No more records available.")
                break

            # Apply Python filter if configured
            if needs_python_filter:
                original_count = len(batch_records)
                batch_records = apply_parent_id_filter(batch_records, filter_config)
                filtered_count = len(batch_records)
                logger.info(
                    f"Batch {batch_num}: Filtered {original_count} → {filtered_count} records"
                )

            # Accumulate records
            accumulated_records.extend(batch_records)

            # Update counters
            offset += batch_size
            batch_num += 1

            # For exact mode without filters: trim if we exceeded target
            if not needs_python_filter and target_mode == 'exact':
                if len(accumulated_records) >= target_count:
                    break

            # For minimum mode: stop when we have at least target count
            if target_mode == 'minimum' and len(accumulated_records) >= target_count:
                logger.info(
                    f"Minimum target {target_count} reached "
                    f"(have {len(accumulated_records)} records). Stopping pagination."
                )
                break

        except Exception as e:
            logger.error(f"Error during pagination at batch {batch_num}: {e}")
            raise

    # Apply target mode trimming for 'exact' mode
    total_fetched = len(accumulated_records)

    if target_mode == 'exact' and total_fetched > target_count:
        logger.info(f"Trimming {total_fetched} records to exactly {target_count}")
        accumulated_records = accumulated_records[:target_count]

    final_count = len(accumulated_records)

    logger.info("=" * 70)
    logger.info("PAGINATION SUMMARY")
    logger.info("=" * 70)
    logger.info(f"Total batches fetched: {batch_num - 1}")
    logger.info(f"Target count: {target_count}")
    logger.info(f"Final record count: {final_count}")

    if final_count < target_count:
        logger.warning(
            f"Fetched only {final_count} records (target was {target_count}). "
            f"All available records have been retrieved."
        )

    # Write merged CSV
    from datetime import datetime
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    merged_csv_path = output_dir / f"attachments_{timestamp}_paginated.csv"

    logger.info(f"Writing merged CSV: {merged_csv_path}")

    with merged_csv_path.open('w', encoding='utf-8', newline='') as f:
        if csv_fieldnames:
            writer = csv.DictWriter(f, fieldnames=csv_fieldnames)
            writer.writeheader()
            writer.writerows(accumulated_records)
        else:
            logger.warning("No fieldnames captured, empty CSV written")

    logger.info(f"Merged CSV created with {final_count} records")

    return merged_csv_path


def process_csv_records_workflow(
    org_alias: str,
    output_dir: Path,
    records_dir: Path,
    batch_size: int = 100,
    download: bool = True
) -> Dict[str, any]:
    """
    Process CSV files containing record IDs and download their attachments.

    This workflow:
    1. Discovers CSV files in records_dir
    2. Extracts record IDs from each CSV's 'Id' column
    3. Batches IDs (default: 100 per batch) to respect SOQL length limits
    4. Queries attachments for each batch using WHERE ParentId IN (...)
    5. Downloads attachments organized by CSV filename

    Args:
        org_alias: Salesforce org alias for authentication
        output_dir: Base output directory (subdirectories created per CSV)
        records_dir: Directory containing CSV files with record IDs
        batch_size: Number of ParentIds per SOQL query (default: 100)
        download: Whether to download files after querying (default: True)

    Returns:
        Dictionary with processing statistics:
        {
            'total_csv_files': int,
            'total_records': int,
            'total_batches': int,
            'total_attachments': int,
            'per_csv': [{'csv_name': str, 'records': int, ...}, ...]
        }

    Raises:
        FileNotFoundError: If records_dir doesn't exist
        ValueError: If no CSV files found or CSV missing 'Id' column
        RuntimeError: If query or download fails
    """
    from src.csv_processor import process_records_directory
    from src.filters import ParentIdFilter, build_soql_where_clause
    from datetime import datetime

    logger.info("=" * 70)
    logger.info("CSV-BASED ATTACHMENT EXTRACTION WORKFLOW")
    logger.info("=" * 70)
    logger.info(f"Org: {org_alias}")
    logger.info(f"Records directory: {records_dir.absolute()}")
    logger.info(f"Output directory: {output_dir.absolute()}")
    logger.info(f"Batch size: {batch_size}")
    logger.info(f"Download enabled: {download}")
    logger.info("")

    # Process CSV files - validates, extracts IDs, creates batches
    csv_records = process_records_directory(records_dir, batch_size)

    # Statistics tracking
    stats = {
        'total_csv_files': len(csv_records),
        'total_records': 0,
        'total_batches': 0,
        'total_attachments': 0,
        'per_csv': []
    }

    failed_files = []

    # Process each CSV file
    for csv_idx, csv_info in enumerate(csv_records, start=1):
        logger.info("=" * 70)
        logger.info(f"PROCESSING CSV {csv_idx}/{len(csv_records)}: {csv_info.csv_name}.csv")
        logger.info("=" * 70)
        logger.info(f"Records: {csv_info.total_records}")
        logger.info(f"Batches: {csv_info.total_batches}")
        logger.info("")

        try:
            # Create output subdirectories for this CSV
            csv_output_dir = output_dir / csv_info.csv_name
            csv_metadata_dir = csv_output_dir / 'metadata'
            csv_files_dir = csv_output_dir / 'files'

            csv_metadata_dir.mkdir(parents=True, exist_ok=True)
            csv_files_dir.mkdir(parents=True, exist_ok=True)

            logger.info(f"Output directories:")
            logger.info(f"  Metadata: {csv_metadata_dir}")
            logger.info(f"  Files: {csv_files_dir}")
            logger.info("")

            # Query attachments for each batch
            batch_csv_paths = []
            accumulated_attachments = []
            csv_fieldnames = None

            for batch_idx, id_batch in enumerate(csv_info.id_batches):
                logger.info(f"Batch {batch_idx + 1}/{csv_info.total_batches}: Querying {len(id_batch)} ParentId(s)")

                # Build WHERE clause using existing filters module
                filter_config = ParentIdFilter(
                    prefixes=[],
                    exact_ids=id_batch,
                    strategy='soql'
                )
                where_clause = build_soql_where_clause(filter_config)

                # Query attachments for this batch
                batch_csv_path = run_query_script_with_filter(
                    org_alias=org_alias,
                    output_dir=csv_metadata_dir,
                    where_clause=where_clause
                )

                batch_csv_paths.append(batch_csv_path)

                # Read batch results
                with batch_csv_path.open('r', encoding='utf-8') as f:
                    reader = csv.DictReader(f)

                    # Capture fieldnames from first batch
                    if csv_fieldnames is None:
                        csv_fieldnames = reader.fieldnames
                        logger.debug(f"Captured fieldnames: {csv_fieldnames}")

                    # Accumulate all attachments
                    batch_attachments = list(reader)
                    accumulated_attachments.extend(batch_attachments)

                    logger.info(f"Batch {batch_idx + 1}/{csv_info.total_batches}: Found {len(batch_attachments)} attachment(s)")

                logger.info("")

            # Merge all batch results into single CSV
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            merged_csv_path = csv_metadata_dir / f"attachments_{timestamp}_merged.csv"

            logger.info(f"Merging {len(batch_csv_paths)} batch CSV(s) into: {merged_csv_path.name}")

            with merged_csv_path.open('w', encoding='utf-8', newline='') as f:
                if csv_fieldnames:
                    writer = csv.DictWriter(f, fieldnames=csv_fieldnames)
                    writer.writeheader()
                    writer.writerows(accumulated_attachments)
                else:
                    logger.warning("No fieldnames captured - writing empty CSV")

            logger.info(f"Merged CSV created with {len(accumulated_attachments)} attachment(s)")
            logger.info("")

            # Download attachments if enabled
            downloaded_count = 0
            if download and len(accumulated_attachments) > 0:
                logger.info(f"Downloading {len(accumulated_attachments)} attachment(s) to: {csv_files_dir}")
                logger.info("")

                try:
                    downloaded_count = download_attachments(
                        org_alias=org_alias,
                        metadata_csv=merged_csv_path,
                        output_dir=csv_files_dir,
                        filter_config=None  # No additional filtering needed
                    )
                    logger.info(f"Downloaded {downloaded_count}/{len(accumulated_attachments)} file(s)")
                except Exception as e:
                    logger.error(f"Download failed for {csv_info.csv_name}: {e}")
                    # Continue processing other CSVs even if download fails
            elif download and len(accumulated_attachments) == 0:
                logger.info("No attachments to download")
            else:
                logger.info("Download skipped (download=False)")

            # Record statistics for this CSV
            csv_stats = {
                'csv_name': csv_info.csv_name,
                'records': csv_info.total_records,
                'batches': csv_info.total_batches,
                'attachments': len(accumulated_attachments),
                'downloaded': downloaded_count,
                'output_dir': str(csv_output_dir)
            }
            stats['per_csv'].append(csv_stats)
            stats['total_records'] += csv_info.total_records
            stats['total_batches'] += csv_info.total_batches
            stats['total_attachments'] += len(accumulated_attachments)

            logger.info(f"✓ Completed {csv_info.csv_name}.csv")
            logger.info("")

        except Exception as e:
            logger.error(f"✗ Failed to process {csv_info.csv_name}.csv: {e}")
            failed_files.append(csv_info.csv_name)
            logger.info("")
            # Continue processing other CSV files

    # Final summary
    logger.info("=" * 70)
    logger.info("WORKFLOW SUMMARY")
    logger.info("=" * 70)
    logger.info(f"Total CSV files: {stats['total_csv_files']}")
    logger.info(f"Total records: {stats['total_records']}")
    logger.info(f"Total batches executed: {stats['total_batches']}")
    logger.info(f"Total attachments found: {stats['total_attachments']}")

    if failed_files:
        logger.warning(f"Failed to process {len(failed_files)} file(s): {', '.join(failed_files)}")
    else:
        logger.info("All CSV files processed successfully!")

    logger.info("")

    return stats


def validate_metadata_csv(csv_path: Path) -> tuple[bool, Optional[str]]:
    """
    Validate that a CSV file has the required structure for attachment metadata.

    Checks that the file exists, is readable, and contains the required columns
    (Id, Name) needed for downloading attachments.

    Args:
        csv_path: Path to the CSV file to validate

    Returns:
        Tuple of (is_valid, error_message)
        - is_valid: True if CSV is valid, False otherwise
        - error_message: None if valid, error description if invalid

    Example:
        >>> is_valid, error = validate_metadata_csv(Path("data.csv"))
        >>> if not is_valid:
        ...     print(f"Invalid CSV: {error}")
    """
    # Check file exists
    if not csv_path.exists():
        return False, f"CSV file not found: {csv_path.absolute()}"

    # Check file is readable
    if not csv_path.is_file():
        return False, f"Path is not a file: {csv_path.absolute()}"

    try:
        # Try to read and validate structure
        with csv_path.open('r', encoding='utf-8') as f:
            reader = csv.DictReader(f)

            # Check if file is empty
            fieldnames = reader.fieldnames
            if not fieldnames:
                return False, "CSV file is empty or has no header row"

            # Check required columns
            required_columns = ['Id', 'Name']
            recommended_columns = ['ParentId']  # Recommended for filtering

            missing_required = [col for col in required_columns if col not in fieldnames]
            missing_recommended = [col for col in recommended_columns if col not in fieldnames]

            if missing_required:
                return False, (
                    f"CSV is missing required columns: {', '.join(missing_required)}. "
                    f"Found columns: {', '.join(fieldnames)}. "
                    f"Expected format from Salesforce query with columns: Id, Name, ContentType, BodyLength, ParentId, etc."
                )

            # Warn about missing recommended columns (don't fail validation)
            if missing_recommended:
                logger.warning(
                    f"CSV is missing recommended column(s): {', '.join(missing_recommended)}. "
                    f"Filtering by ParentId will not be possible."
                )

            # Check if CSV has any data rows
            try:
                first_row = next(reader)
                # Validate that Id and Name are not empty in first row
                if not first_row.get('Id') or not first_row.get('Id').strip():
                    return False, "CSV has empty 'Id' field in first data row"
                if not first_row.get('Name') or not first_row.get('Name').strip():
                    return False, "CSV has empty 'Name' field in first data row"
            except StopIteration:
                return False, "CSV has header but no data rows"

        return True, None

    except UnicodeDecodeError:
        return False, f"CSV file has invalid encoding. Expected UTF-8."
    except csv.Error as e:
        return False, f"CSV parsing error: {e}"
    except Exception as e:
        return False, f"Unexpected error reading CSV: {e}"


def main():
    """
    Main entry point for the Salesforce Attachments Extract workflow.

    Orchestrates the complete process of querying and downloading attachments.
    Loads configuration from .env file if present, with CLI arguments taking precedence.
    """
    import argparse

    # Load environment variables from .env file (if present)
    load_dotenv()

    # Get environment variables with defaults
    env_org_alias = os.getenv('SF_ORG_ALIAS')
    env_output_dir = os.getenv('OUTPUT_DIR', './output')
    env_log_file = os.getenv('LOG_FILE', './logs/download.log')
    env_chunk_size = os.getenv('CHUNK_SIZE', '8192')
    env_metadata_csv = os.getenv('METADATA_CSV')

    # Filter configuration from .env
    env_parent_id_prefix = os.getenv('PARENT_ID_PREFIX')
    env_parent_ids = os.getenv('PARENT_IDS')
    env_filter_strategy = os.getenv('FILTER_STRATEGY', 'python')

    # Query limit from .env
    env_query_limit = os.getenv('QUERY_LIMIT', '100')

    # Pagination configuration from .env
    env_target_count = os.getenv('TARGET_COUNT', '')
    env_target_mode = os.getenv('TARGET_MODE', 'exact')

    # CSV Records processing configuration from .env
    env_records_dir = os.getenv('RECORDS_DIR')
    env_batch_size = os.getenv('BATCH_SIZE', '100')

    # Validate and convert CHUNK_SIZE to int
    try:
        chunk_size = int(env_chunk_size)
    except ValueError:
        logger.warning(f"Invalid CHUNK_SIZE value '{env_chunk_size}', using default 8192")
        chunk_size = 8192

    # Validate and convert QUERY_LIMIT to int
    try:
        default_query_limit = int(env_query_limit)
    except ValueError:
        logger.warning(f"Invalid QUERY_LIMIT value '{env_query_limit}', using default 100")
        default_query_limit = 100

    # Validate and convert TARGET_COUNT to int (optional)
    default_target_count = None
    if env_target_count and env_target_count.strip():
        try:
            default_target_count = int(env_target_count)
            if default_target_count <= 0:
                logger.warning(f"TARGET_COUNT must be positive, got {default_target_count}. Pagination disabled.")
                default_target_count = None
        except ValueError:
            logger.warning(f"Invalid TARGET_COUNT value '{env_target_count}', pagination disabled")
            default_target_count = None

    # Validate TARGET_MODE
    if env_target_mode not in ['exact', 'minimum']:
        logger.warning(f"Invalid TARGET_MODE value '{env_target_mode}', using default 'exact'")
        env_target_mode = 'exact'

    # Validate and convert BATCH_SIZE to int
    default_batch_size = 100
    try:
        default_batch_size = int(env_batch_size)
        if default_batch_size < 1:
            logger.warning(f"BATCH_SIZE must be at least 1, got {default_batch_size}. Using default 100.")
            default_batch_size = 100
    except ValueError:
        logger.warning(f"Invalid BATCH_SIZE value '{env_batch_size}', using default 100")
        default_batch_size = 100

    parser = argparse.ArgumentParser(
        description='Query and download Salesforce attachments (POC)'
    )
    parser.add_argument(
        '--org',
        type=str,
        default=env_org_alias,
        help=f'Salesforce org alias (default: {env_org_alias or "use default org"})'
    )
    parser.add_argument(
        '--output',
        type=Path,
        default=Path(env_output_dir),
        help=f'Base output directory (default: {env_output_dir})'
    )
    parser.add_argument(
        '--skip-query',
        action='store_true',
        help='(Deprecated) Skip query step - use --metadata instead'
    )
    parser.add_argument(
        '--metadata',
        type=lambda p: Path(p) if p else None,
        default=Path(env_metadata_csv) if env_metadata_csv and env_metadata_csv.strip() else None,
        help='Path to existing metadata CSV (skips Salesforce query)'
    )
    parser.add_argument(
        '--parent-id-prefix',
        type=str,
        default=env_parent_id_prefix,
        help='Comma-separated ParentId prefixes to filter (e.g., "aBo,001")'
    )
    parser.add_argument(
        '--parent-ids',
        type=str,
        default=env_parent_ids,
        help='Comma-separated specific ParentIds to filter'
    )
    parser.add_argument(
        '--filter-strategy',
        type=str,
        choices=['python', 'soql'],
        default=env_filter_strategy,
        help='Filtering strategy: python (post-query) or soql (in-query)'
    )
    parser.add_argument(
        '--query-limit',
        type=int,
        default=default_query_limit,
        help=f'Maximum number of records to query per batch (default: {default_query_limit})'
    )
    parser.add_argument(
        '--target-count',
        type=int,
        default=default_target_count,
        help='Target number of attachments to retrieve using pagination (optional)'
    )
    parser.add_argument(
        '--target-mode',
        type=str,
        choices=['exact', 'minimum'],
        default=env_target_mode,
        help='Target mode: exact (trim to TARGET_COUNT) or minimum (at least TARGET_COUNT)'
    )
    parser.add_argument(
        '--records-dir',
        type=Path,
        default=None,
        help='Directory containing CSV files with record IDs (column: Id). Enables CSV-based processing mode.'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=default_batch_size,
        help=f'Number of ParentIds per SOQL query batch for CSV mode (default: {default_batch_size})'
    )

    args = parser.parse_args()

    # Setup logging (use env variable for log file path)
    log_file = Path(env_log_file)
    setup_logging(log_file)

    logger.info("=" * 70)
    logger.info("SALESFORCE ATTACHMENTS EXTRACT - POC")
    logger.info("=" * 70)

    # Automatic mode detection for CSV-records workflow
    # Mode is activated if:
    # 1. --records-dir is provided via CLI, OR
    # 2. RECORDS_DIR is set in .env and the directory exists
    use_csv_mode = False
    records_dir = None

    if args.records_dir:
        # CLI argument takes precedence
        records_dir = args.records_dir
        use_csv_mode = True
        logger.info(f"CSV-records mode activated via --records-dir: {records_dir}")
    elif env_records_dir:
        # Check if .env RECORDS_DIR exists
        records_dir = Path(env_records_dir)
        if records_dir.exists() and records_dir.is_dir():
            use_csv_mode = True
            logger.info(f"CSV-records mode activated via RECORDS_DIR env: {records_dir}")
        else:
            logger.debug(f"RECORDS_DIR set but directory not found: {records_dir}")

    try:
        # CSV-based workflow (new mode)
        if use_csv_mode:
            logger.info("")
            stats = process_csv_records_workflow(
                org_alias=args.org,
                output_dir=args.output,
                records_dir=records_dir,
                batch_size=args.batch_size,
                download=True
            )

            # Final summary
            logger.info("=" * 70)
            logger.info("WORKFLOW COMPLETE")
            logger.info("=" * 70)
            logger.info(f"CSV files processed: {stats['total_csv_files']}")
            logger.info(f"Total records: {stats['total_records']}")
            logger.info(f"Total attachments: {stats['total_attachments']}")
            logger.info("")

            sys.exit(0)

        # Standard workflow (existing behavior)
        # Parse filter configuration
        filter_config = parse_filter_config(
            prefix_str=args.parent_id_prefix,
            ids_str=args.parent_ids,
            strategy=args.filter_strategy
        )

        if filter_config:
            logger.info(f"Filter configuration: {filter_config}")
        else:
            logger.info("No filtering configured - will download all attachments")

        # Step 1: Query attachments (or use provided CSV)
        if args.metadata:
            # User provided existing CSV - validate and use it
            logger.info("=" * 70)
            logger.info("USING PROVIDED METADATA CSV")
            logger.info("=" * 70)
            logger.info(f"CSV file: {args.metadata}")

            # Validate CSV structure
            is_valid, error_msg = validate_metadata_csv(args.metadata)
            if not is_valid:
                logger.error("=" * 70)
                logger.error("CSV VALIDATION FAILED")
                logger.error("=" * 70)
                logger.error(f"Error: {error_msg}")
                logger.error("")
                logger.error("Please ensure your CSV file:")
                logger.error("  1. Exists and is readable")
                logger.error("  2. Has a header row with column names")
                logger.error("  3. Contains required columns: 'Id' and 'Name'")
                logger.error("  4. Contains recommended column: 'ParentId' (needed for filtering)")
                logger.error("  5. Has at least one data row")
                logger.error("  6. Is UTF-8 encoded")
                logger.error("")
                logger.error("You can generate a valid CSV by querying Salesforce:")
                logger.error(f"  python {sys.argv[0]} --org your-org --query-limit 100")
                sys.exit(1)

            csv_path = args.metadata
            logger.info("✓ CSV validation passed")
            logger.info(f"✓ Ready to download attachments from metadata")
        else:
            # No CSV provided - query from Salesforce
            metadata_dir = args.output / 'metadata'

            # Check if pagination is configured
            if args.target_count and args.target_count > 0:
                logger.info(f"Pagination enabled: target={args.target_count}, mode={args.target_mode}")
                csv_path = run_paginated_query(
                    org_alias=args.org,
                    output_dir=metadata_dir,
                    target_count=args.target_count,
                    batch_size=args.query_limit,
                    target_mode=args.target_mode,
                    filter_config=filter_config
                )
            else:
                # Legacy single query mode
                logger.info("Single query mode (pagination disabled)")
                csv_path = run_query_script(args.org, metadata_dir, args.query_limit)

            logger.info(f"Generated metadata: {csv_path}")

        # Step 2: Download files
        files_dir = args.output / 'files'

        # If pagination was used with Python filters, filters were already applied
        # Don't apply them again in the downloader
        downloader_filter_config = filter_config
        if args.target_count and args.target_count > 0:
            if filter_config and filter_config.strategy == 'python':
                logger.info("Python filters already applied during pagination - skipping filter in downloader")
                downloader_filter_config = None

        stats = download_attachments(
            metadata_csv=csv_path,
            output_dir=files_dir,
            org_alias=args.org,
            chunk_size=chunk_size,
            filter_config=downloader_filter_config
        )

        # Final summary
        logger.info("=" * 70)
        logger.info("WORKFLOW COMPLETE")
        logger.info("=" * 70)
        logger.info(f"Metadata: {csv_path}")
        logger.info(f"Files: {files_dir}")
        logger.info(f"Success: {stats['success']}/{stats['total']}")

        sys.exit(0 if stats['failed'] == 0 else 1)

    except Exception as e:
        logger.error(f"Fatal error: {e}", exc_info=True)
        sys.exit(2)


if __name__ == '__main__':
    main()
