#!/usr/bin/env python3
"""
Salesforce Attachments Extract - Main Entry Point

Runs the complete workflow:
1. Query attachments using sf CLI
2. Download attachment files using REST API
"""

import os
import sys
import subprocess
import logging
from pathlib import Path
from dotenv import load_dotenv

from src.downloader import download_attachments, setup_logging
from src.filters import parse_filter_config

logger = logging.getLogger(__name__)


def run_query_script(
    org_alias: str,
    output_dir: Path,
    query_limit: int = 100
) -> Path:
    """
    Run the bash script to query attachments.

    Args:
        org_alias: Salesforce org alias
        output_dir: Directory to save metadata CSV
        query_limit: Maximum number of records to query (default: 100)

    Returns:
        Path to the generated CSV file

    Raises:
        RuntimeError: If query script fails
        FileNotFoundError: If script or CSV not found
    """
    logger.info("Running attachment query script...")
    logger.info(f"Query limit: {query_limit}")

    script_path = Path(__file__).parent / 'scripts' / 'query_attachments.sh'

    if not script_path.exists():
        raise FileNotFoundError(
            f"Query script not found at path: {script_path.absolute()}. "
            f"Please ensure the scripts directory and query_attachments.sh exist."
        )

    # Run script with query limit parameter
    result = subprocess.run(
        ['bash', str(script_path), org_alias, str(output_dir), str(query_limit)],
        capture_output=True,
        text=True
    )

    if result.returncode != 0:
        logger.error(f"Query script failed: {result.stderr}")
        raise RuntimeError("Failed to query attachments")

    logger.info(result.stdout)

    # Find the generated CSV file (most recent in output_dir)
    csv_files = sorted(output_dir.glob('attachments_*.csv'), key=lambda p: p.stat().st_mtime)

    if not csv_files:
        raise FileNotFoundError(
            f"No CSV file generated by query script in directory: {output_dir.absolute()}. "
            f"The query script may have failed or no attachments were found."
        )

    return csv_files[-1]  # Return most recent


def main():
    """
    Main entry point for the Salesforce Attachments Extract workflow.

    Orchestrates the complete process of querying and downloading attachments.
    Loads configuration from .env file if present, with CLI arguments taking precedence.
    """
    import argparse

    # Load environment variables from .env file (if present)
    load_dotenv()

    # Get environment variables with defaults
    env_org_alias = os.getenv('SF_ORG_ALIAS')
    env_output_dir = os.getenv('OUTPUT_DIR', './output')
    env_log_file = os.getenv('LOG_FILE', './logs/download.log')
    env_chunk_size = os.getenv('CHUNK_SIZE', '8192')

    # Filter configuration from .env
    env_parent_id_prefix = os.getenv('PARENT_ID_PREFIX')
    env_parent_ids = os.getenv('PARENT_IDS')
    env_filter_strategy = os.getenv('FILTER_STRATEGY', 'python')

    # Query limit from .env
    env_query_limit = os.getenv('QUERY_LIMIT', '100')

    # Validate and convert CHUNK_SIZE to int
    try:
        chunk_size = int(env_chunk_size)
    except ValueError:
        logger.warning(f"Invalid CHUNK_SIZE value '{env_chunk_size}', using default 8192")
        chunk_size = 8192

    # Validate and convert QUERY_LIMIT to int
    try:
        default_query_limit = int(env_query_limit)
    except ValueError:
        logger.warning(f"Invalid QUERY_LIMIT value '{env_query_limit}', using default 100")
        default_query_limit = 100

    parser = argparse.ArgumentParser(
        description='Query and download Salesforce attachments (POC)'
    )
    parser.add_argument(
        '--org',
        type=str,
        default=env_org_alias,
        help=f'Salesforce org alias (default: {env_org_alias or "use default org"})'
    )
    parser.add_argument(
        '--output',
        type=Path,
        default=Path(env_output_dir),
        help=f'Base output directory (default: {env_output_dir})'
    )
    parser.add_argument(
        '--skip-query',
        action='store_true',
        help='Skip query step and use existing metadata CSV'
    )
    parser.add_argument(
        '--metadata',
        type=Path,
        help='Path to existing metadata CSV (used with --skip-query)'
    )
    parser.add_argument(
        '--parent-id-prefix',
        type=str,
        default=env_parent_id_prefix,
        help='Comma-separated ParentId prefixes to filter (e.g., "aBo,001")'
    )
    parser.add_argument(
        '--parent-ids',
        type=str,
        default=env_parent_ids,
        help='Comma-separated specific ParentIds to filter'
    )
    parser.add_argument(
        '--filter-strategy',
        type=str,
        choices=['python', 'soql'],
        default=env_filter_strategy,
        help='Filtering strategy: python (post-query) or soql (in-query)'
    )
    parser.add_argument(
        '--query-limit',
        type=int,
        default=default_query_limit,
        help=f'Maximum number of records to query (default: {default_query_limit})'
    )

    args = parser.parse_args()

    # Setup logging (use env variable for log file path)
    log_file = Path(env_log_file)
    setup_logging(log_file)

    logger.info("=" * 70)
    logger.info("SALESFORCE ATTACHMENTS EXTRACT - POC")
    logger.info("=" * 70)

    try:
        # Parse filter configuration
        filter_config = parse_filter_config(
            prefix_str=args.parent_id_prefix,
            ids_str=args.parent_ids,
            strategy=args.filter_strategy
        )

        if filter_config:
            logger.info(f"Filter configuration: {filter_config}")
        else:
            logger.info("No filtering configured - will download all attachments")

        # Step 1: Query attachments (unless skipped)
        if args.skip_query:
            if not args.metadata or not args.metadata.exists():
                logger.error("--skip-query requires valid --metadata path")
                sys.exit(1)
            csv_path = args.metadata
            logger.info(f"Using existing metadata: {csv_path}")
        else:
            metadata_dir = args.output / 'metadata'
            csv_path = run_query_script(args.org, metadata_dir, args.query_limit)
            logger.info(f"Generated metadata: {csv_path}")

        # Step 2: Download files
        files_dir = args.output / 'files'
        stats = download_attachments(
            metadata_csv=csv_path,
            output_dir=files_dir,
            org_alias=args.org,
            chunk_size=chunk_size,
            filter_config=filter_config
        )

        # Final summary
        logger.info("=" * 70)
        logger.info("WORKFLOW COMPLETE")
        logger.info("=" * 70)
        logger.info(f"Metadata: {csv_path}")
        logger.info(f"Files: {files_dir}")
        logger.info(f"Success: {stats['success']}/{stats['total']}")

        sys.exit(0 if stats['failed'] == 0 else 1)

    except Exception as e:
        logger.error(f"Fatal error: {e}", exc_info=True)
        sys.exit(2)


if __name__ == '__main__':
    main()
